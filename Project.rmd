---
output: html_document
---
### Practical Machine Learning - Class Project

```{r setoptions,echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, results = "hide")
```

__Summary__

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible 
to collect a large amount of data about personal activity relatively inexpensively.
These type of devices are part of the quantified self movement â€“ a group of 
enthusiasts who take measurements about themselves regularly to improve their 
health, to find patterns in their behavior, or because they are tech geeks. One 
thing that people regularly do is quantify how much of a particular activity 
they do, but they rarely quantify how well they do it.

Use the data from accelerometers on the belt, forearm, arm, and dumbell of 6 
participants. They were asked to perform barbell lifts correctly and incorrectly 
in 5 different ways.

The goal of this project is to predict the manner in which participants did the exercise. 
This is the "classe" variable in the training set. You may use any of the other 
variables to predict with. You should create a report describing how you built 
your model, how you used cross validation, what you think the expected out of 
sample error is, and why you made the choices you did. You will also use your 
prediction model to predict 20 different test cases.

__Loading And Cleaning Data__

The data for this project come from source: http://groupware.les.inf.puc-rio.br/har. 
If you use the document you created for this class for any purpose please cite them 
as they have been very generous in allowing their data to be used for this kind 
of assignment. 

First, download the data from cloudfront.

```{r downloaddata, echo=TRUE}
if (!file.exists("pml-training.csv"))
{
  FileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(FileURL,destfile="pml-training.csv", method="curl")
}

if (!file.exists("pml-testing.csv"))
{
  FileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(FileURL,destfile="pml-testing.csv", method="curl")
}

```

Second, read the data into R. 

```{r readdata, echo=TRUE, cache=TRUE}
pml.training <- read.csv("pml-training.csv")
pml.testing <- read.csv("pml-testing.csv")
```

Third, sample the data to be sure it's what is expected.

```{r verifydata, echo=TRUE}
dim(pml.training)
dim(pml.testing)
```

_The training dataset has `r dim(pml.training)` observations._
_The testing dataset has `r dim(pml.testing)` observations._

Fourth, eliminate the columns in the dataset that don't pertain to our analysis. 
Here, we are removing columns without observations (NA or empty values).

```{r reducedata, echo=TRUE}
pml.na <- apply(pml.training, 2, function(x) {sum(is.na(x))})
pml.training <- pml.training[,which(pml.na == 0)]
pml.testing  <- pml.testing[,which(pml.na == 0)]

pml.empty <- apply(pml.training, 2, function(x) sum((x=="")))
pml.training <- pml.training[,which(pml.empty == 0)]
pml.testing  <- pml.testing[,which(pml.empty == 0)]
```

Fifth, remove the eight columns at the beginning of the datset that pertain to 
names, timestamps, or unknown values.  After these columns are remvoed, we are 
left with only measurements for pitch, yaw, gyro, acceleration, etc., that we 
can use to model and make predictions.

```{r reducecolumns, echo=TRUE}
pml.training[,c("X")] <- NULL
pml.training[,c("user_name")] <- NULL
pml.training[,c("raw_timestamp_part_1")] <- NULL
pml.training[,c("raw_timestamp_part_2")] <- NULL
pml.training[,c("cvtd_timestamp")] <- NULL
pml.training[,c("new_window")] <- NULL
pml.training[,c("num_window")] <- NULL

pml.testing[,c("X")] <- NULL
pml.testing[,c("user_name")] <- NULL
pml.testing[,c("raw_timestamp_part_1")] <- NULL
pml.testing[,c("raw_timestamp_part_2")] <- NULL
pml.testing[,c("cvtd_timestamp")] <- NULL
pml.testing[,c("new_window")] <- NULL
pml.testing[,c("num_window")] <- NULL
```

_The reduced dataset now has `r dim(pml.training)` observations._

__Building A Model__

The pml.training data will be used to train our model. The first thing we do is
split the pml.training data into two sets using a 75% to 25% ratio.  The sub75.train 
data will be used to build our model.  The sub25.train data will be used for cross 
validation in order to further test our fitted model.

```{r datasplitting, echo=TRUE}
library(caret)
in.pml.training <- createDataPartition(pml.training$classe, p=0.75, list = FALSE)
sub75.train <- pml.training[in.pml.training,]
sub25.train <- pml.training[-in.pml.training,]
```

_The sub75.train dataset has `r dim(sub75.train)` observations._
_The sub25.train dataset has `r dim(sub25.train)` observations._

Random Fmorests was selected as a learning method to correlate the construction of 
multiple decision trees and being able to decide a final predicted outcome across 
all the trees.

Other models eliminated were Trees (too much data to split into groups manually), 
Bagging (could be used with Trees but Bagging is extended in Random 
Forests), and Boosting (since our predictors are actual device measurements they 
don't seem weak enough to benefit from Boosting).

Now, we'll train our model using the randomForest function.  This function was 
selected due to it's speed and performance fitting our model using every value 
except "classe".

```{r modelfit, echo=TRUE}
library(randomForest)
set.seed(2014)
modelFit <- randomForest(classe ~ ., data = sub75.train)
modelFit
```

This model is well balanced and has a very small OOB estimate of 0.47%.

__Cross Validation__

Cross Validation is used determine the accuracy of the model we fitted using Random 
Forests.  Here we use the sub25.train data, which represents unseen data, with 
our model.  Using the sub25.train data, we can estimate an out of sample error and 
determine the appropriate confidence level in predicting future values for "classe".

```{r crossvalidate, echo=TRUE}
pred = predict(modelFit, sub25.train)
confusionMatrix(sub25.train$classe, pred)
```

The overall accuracy rate is computed, along with a 95 percent confidence interval.

Our model produced a 99.37% accuracy rate.  

Within the 95th percentile, our confidence interval is 99.1% to 99.6%

__Predicting "Classe" Values__

Since confidence in the selected model is high, we can now use the 
pml-testing data set provided with the assignment to predict how likely 
participants will be to perform the barbell exercise correctly and with good form.

```{r predicttest, echo=TRUE}
predTest <- predict(modelFit, pml.testing)
predTest
```

__Conclusion__

The model predictions were very accurate.  After applying the machine learning 
algorithm to the 20 test cases and submitting the data to the programming assignment 
for automated grading, I received 20 out of 20 submssions correct.

The grading score matches the confidence interval of the model seen earlier in 
this report.

_99.6% confidence interval * 20 submssions = 19.92 correct answers._

__Outputting the prediction results to individual text files__

The assignment provided this script to output the model predictions to multiple 
test files for submsssion and grading.

```{r outputpredictions, echo=TRUE}
pml_write_files = function (x, directory = "solutions") {
  dir.create (directory)

  n = length(x)
  for(i in 1:n){
    filename = paste0 ("problem_id_",i,".txt")
    filename = file.path (directory, filename)
    write.table (x[i], file=filename, quote=FALSE, row.names=FALSE, col.names=FALSE)
  }
}

pml_write_files (predTest)
```

